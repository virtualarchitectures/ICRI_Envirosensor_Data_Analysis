{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ICRI Envirosensor Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Download dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import json\n",
    "import json\n",
    "\n",
    "#Import ijson to read large json files iteratively\n",
    "import ijson\n",
    "\n",
    "#Import Time module\n",
    "import time\n",
    "\n",
    "#Import Apache Spark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import *\n",
    "\n",
    "from pyspark.sql import functions\n",
    "\n",
    "#Import Pandas for data analysis \n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from pandas.io.json import json_normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Identify data location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\\Envirosensor_TEST.json\n"
     ]
    }
   ],
   "source": [
    "from os import path\n",
    "\n",
    "#In windows the 'r' preceding the file name string indcates that it is a raw sting so that slashes are interpreted correctly\n",
    "#filename = path.expanduser(r'data\\here_east_envirosensors.json')\n",
    "\n",
    "#Test file\n",
    "filename = path.expanduser(r'data\\Envirosensor_TEST.json')\n",
    "\n",
    "#Check that the name has been assigned to the variable correctly\n",
    "print(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Check file size in KB to ensure it can be loaded into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.390625"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path.getsize(filename) / (1<<10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Parse JSON data iteratively with ijson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time to complete: 0.03s\n",
      "\n",
      "{ \"DeviceID\": \"8010\", \"DeviceType\": \"Envirosensor\", \"Event\": \"event\", \"Time\": \"2018-06-03 20:40:41.629620\", \"Data\": { \"TMP\": \"36.187\", \"OPT\": \"4.17\", \"BAT\": \"36.58\", \"HDT\": \"36.32\", \"BAR\": \"1016.17\", \"HDH\": \"22.65\" } }\n"
     ]
    }
   ],
   "source": [
    "#Define function to iteratively parse json file\n",
    "def iterativeParse(json):\n",
    "    with open(json, 'r') as json_file:\n",
    "        #Use the items method in ijson to extract a list of objects specifying the file and key path to to the list\n",
    "        objects = ijson.items(json_file, 'item.data')\n",
    "        #The items fuction returns a generator which we turn into a list of payloads with the list function \n",
    "        parsedData = list(objects)\n",
    "\n",
    "    return parsedData\n",
    "\n",
    "#Time execution of the iterativeParse function and assign the result to payloads variable\n",
    "start = time.time()\n",
    "payloads = iterativeParse(filename)\n",
    "end = time.time()\n",
    "\n",
    "print(f'\\nTime to complete: {end - start:.2f}s\\n')\n",
    "\n",
    "#Check the first item in the payloads list\n",
    "print(payloads[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Display number of payloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sensor Payloads = 11\n"
     ]
    }
   ],
   "source": [
    "#Count items in the payloads list\n",
    "print('Total Sensor Payloads =', len(payloads))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Loop through payloads list and add each to an empty Pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time to complete: 0.03s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Create an empty dataframe\n",
    "df = DataFrame()\n",
    "\n",
    "#Define a function to loop through the collection of payloads and add each one to the dataframe\n",
    "def normaliseDataFrame(x):\n",
    "    tempdf = DataFrame()\n",
    "    for i in x:\n",
    "        row = json_normalize(json.loads(i))\n",
    "        tempdf = tempdf.append(row, sort=False)\n",
    "        \n",
    "        #Display individual rows that are added to the dataframe for debugging\n",
    "        #print(row)\n",
    "        \n",
    "    return tempdf\n",
    "\n",
    "#Time execution of the normaliseDataFrame function and assign the result to payloads variable\n",
    "start = time.time()\n",
    "df = normaliseDataFrame(payloads)\n",
    "end = time.time()\n",
    "\n",
    "print(f'\\nTime to complete: {end - start:.2f}s\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Display dataframe summary information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 11 entries, 0 to 0\n",
      "Data columns (total 10 columns):\n",
      "Data.BAR      11 non-null object\n",
      "Data.BAT      11 non-null object\n",
      "Data.HDH      11 non-null object\n",
      "Data.HDT      11 non-null object\n",
      "Data.OPT      11 non-null object\n",
      "Data.TMP      11 non-null object\n",
      "DeviceID      11 non-null object\n",
      "DeviceType    11 non-null object\n",
      "Event         11 non-null object\n",
      "Time          11 non-null object\n",
      "dtypes: object(10)\n",
      "memory usage: 968.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Display dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data.BAR</th>\n",
       "      <th>Data.BAT</th>\n",
       "      <th>Data.HDH</th>\n",
       "      <th>Data.HDT</th>\n",
       "      <th>Data.OPT</th>\n",
       "      <th>Data.TMP</th>\n",
       "      <th>DeviceID</th>\n",
       "      <th>DeviceType</th>\n",
       "      <th>Event</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1016.17</td>\n",
       "      <td>36.58</td>\n",
       "      <td>22.65</td>\n",
       "      <td>36.32</td>\n",
       "      <td>4.17</td>\n",
       "      <td>36.187</td>\n",
       "      <td>8010</td>\n",
       "      <td>Envirosensor</td>\n",
       "      <td>event</td>\n",
       "      <td>2018-06-03 20:40:41.629620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1016.14</td>\n",
       "      <td>35.06</td>\n",
       "      <td>28.68</td>\n",
       "      <td>34.76</td>\n",
       "      <td>0.00</td>\n",
       "      <td>34.562</td>\n",
       "      <td>8017</td>\n",
       "      <td>Envirosensor</td>\n",
       "      <td>event</td>\n",
       "      <td>2018-06-01 04:22:22.881564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1017.23</td>\n",
       "      <td>35.49</td>\n",
       "      <td>26.23</td>\n",
       "      <td>35.06</td>\n",
       "      <td>6.17</td>\n",
       "      <td>34.781</td>\n",
       "      <td>8008</td>\n",
       "      <td>Envirosensor</td>\n",
       "      <td>event</td>\n",
       "      <td>2018-06-01 13:04:56.152669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1013.88</td>\n",
       "      <td>34.07</td>\n",
       "      <td>21.58</td>\n",
       "      <td>34.14</td>\n",
       "      <td>73.04</td>\n",
       "      <td>33.843</td>\n",
       "      <td>8019</td>\n",
       "      <td>Envirosensor</td>\n",
       "      <td>event</td>\n",
       "      <td>2018-05-29 22:48:18.852665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1017.59</td>\n",
       "      <td>35.05</td>\n",
       "      <td>27.41</td>\n",
       "      <td>34.77</td>\n",
       "      <td>6.67</td>\n",
       "      <td>34.625</td>\n",
       "      <td>8010</td>\n",
       "      <td>Envirosensor</td>\n",
       "      <td>event</td>\n",
       "      <td>2018-06-01 13:19:15.175496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1017.23</td>\n",
       "      <td>36.19</td>\n",
       "      <td>18.90</td>\n",
       "      <td>36.06</td>\n",
       "      <td>4.89</td>\n",
       "      <td>35.906</td>\n",
       "      <td>8004</td>\n",
       "      <td>Envirosensor</td>\n",
       "      <td>event</td>\n",
       "      <td>2018-06-02 23:12:12.230449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1018.17</td>\n",
       "      <td>36.51</td>\n",
       "      <td>21.66</td>\n",
       "      <td>36.07</td>\n",
       "      <td>8.50</td>\n",
       "      <td>35.812</td>\n",
       "      <td>8008</td>\n",
       "      <td>Envirosensor</td>\n",
       "      <td>event</td>\n",
       "      <td>2018-06-03 07:33:16.774434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1013.19</td>\n",
       "      <td>34.93</td>\n",
       "      <td>20.34</td>\n",
       "      <td>35.01</td>\n",
       "      <td>74.00</td>\n",
       "      <td>34.718</td>\n",
       "      <td>8019</td>\n",
       "      <td>Envirosensor</td>\n",
       "      <td>event</td>\n",
       "      <td>2018-05-30 09:25:24.052756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1018.45</td>\n",
       "      <td>35.87</td>\n",
       "      <td>22.71</td>\n",
       "      <td>35.52</td>\n",
       "      <td>108.72</td>\n",
       "      <td>35.281</td>\n",
       "      <td>8005</td>\n",
       "      <td>Envirosensor</td>\n",
       "      <td>event</td>\n",
       "      <td>2018-06-03 00:30:56.678239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1018.45</td>\n",
       "      <td>37.08</td>\n",
       "      <td>17.05</td>\n",
       "      <td>36.84</td>\n",
       "      <td>0.24</td>\n",
       "      <td>36.500</td>\n",
       "      <td>8018</td>\n",
       "      <td>Envirosensor</td>\n",
       "      <td>event</td>\n",
       "      <td>2018-06-03 09:34:58.436492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1018.69</td>\n",
       "      <td>36.04</td>\n",
       "      <td>19.87</td>\n",
       "      <td>35.83</td>\n",
       "      <td>111.92</td>\n",
       "      <td>35.531</td>\n",
       "      <td>8016</td>\n",
       "      <td>Envirosensor</td>\n",
       "      <td>event</td>\n",
       "      <td>2018-06-03 07:40:26.538855</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Data.BAR Data.BAT Data.HDH Data.HDT Data.OPT Data.TMP DeviceID  \\\n",
       "0  1016.17    36.58    22.65    36.32     4.17   36.187     8010   \n",
       "0  1016.14    35.06    28.68    34.76     0.00   34.562     8017   \n",
       "0  1017.23    35.49    26.23    35.06     6.17   34.781     8008   \n",
       "0  1013.88    34.07    21.58    34.14    73.04   33.843     8019   \n",
       "0  1017.59    35.05    27.41    34.77     6.67   34.625     8010   \n",
       "0  1017.23    36.19    18.90    36.06     4.89   35.906     8004   \n",
       "0  1018.17    36.51    21.66    36.07     8.50   35.812     8008   \n",
       "0  1013.19    34.93    20.34    35.01    74.00   34.718     8019   \n",
       "0  1018.45    35.87    22.71    35.52   108.72   35.281     8005   \n",
       "0  1018.45    37.08    17.05    36.84     0.24   36.500     8018   \n",
       "0  1018.69    36.04    19.87    35.83   111.92   35.531     8016   \n",
       "\n",
       "     DeviceType  Event                        Time  \n",
       "0  Envirosensor  event  2018-06-03 20:40:41.629620  \n",
       "0  Envirosensor  event  2018-06-01 04:22:22.881564  \n",
       "0  Envirosensor  event  2018-06-01 13:04:56.152669  \n",
       "0  Envirosensor  event  2018-05-29 22:48:18.852665  \n",
       "0  Envirosensor  event  2018-06-01 13:19:15.175496  \n",
       "0  Envirosensor  event  2018-06-02 23:12:12.230449  \n",
       "0  Envirosensor  event  2018-06-03 07:33:16.774434  \n",
       "0  Envirosensor  event  2018-05-30 09:25:24.052756  \n",
       "0  Envirosensor  event  2018-06-03 00:30:56.678239  \n",
       "0  Envirosensor  event  2018-06-03 09:34:58.436492  \n",
       "0  Envirosensor  event  2018-06-03 07:40:26.538855  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup Apache Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a Spark session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Envirosensor Data Analysis\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse JSON Directly into Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read JSON data directly into Apache Spark data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time to complete: 1.69s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Place JSON into a Spark dataframe and time execution\n",
    "start = time.time()\n",
    "df = spark.read.json(filename, multiLine=True)\n",
    "end = time.time()\n",
    "\n",
    "print(f'\\nTime to complete: {end - start:.2f}s\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+------------+---------+------+--------------------+--------------------+\n",
      "|                data|deviceId|  deviceType|eventType|format|    json_featuretype|           timestamp|\n",
      "+--------------------+--------+------------+---------+------+--------------------+--------------------+\n",
      "|{ \"DeviceID\": \"80...|    8010|Envirosensor|    event|  json|iotp_kfb22t_envir...|2018-06-03T20:40:...|\n",
      "|{ \"DeviceID\": \"80...|    8017|Envirosensor|    event|  json|iotp_kfb22t_envir...|2018-06-01T04:22:...|\n",
      "|{ \"DeviceID\": \"80...|    8008|Envirosensor|    event|  json|iotp_kfb22t_envir...|2018-06-01T13:04:...|\n",
      "|{ \"DeviceID\": \"80...|    8019|Envirosensor|    event|  json|iotp_kfb22t_envir...|2018-05-29T22:48:...|\n",
      "|{ \"DeviceID\": \"80...|    8010|Envirosensor|    event|  json|iotp_kfb22t_envir...|2018-06-01T13:19:...|\n",
      "|{ \"DeviceID\": \"80...|    8004|Envirosensor|    event|  json|iotp_kfb22t_envir...|2018-06-02T23:12:...|\n",
      "|{ \"DeviceID\": \"80...|    8008|Envirosensor|    event|  json|iotp_kfb22t_envir...|2018-06-03T07:33:...|\n",
      "|{ \"DeviceID\": \"80...|    8019|Envirosensor|    event|  json|iotp_kfb22t_envir...|2018-05-30T09:25:...|\n",
      "|{ \"DeviceID\": \"80...|    8005|Envirosensor|    event|  json|iotp_kfb22t_envir...|2018-06-03T00:30:...|\n",
      "|{ \"DeviceID\": \"80...|    8018|Envirosensor|    event|  json|iotp_kfb22t_envir...|2018-06-03T09:34:...|\n",
      "|{ \"DeviceID\": \"80...|    8016|Envirosensor|    event|  json|iotp_kfb22t_envir...|2018-06-03T07:40:...|\n",
      "+--------------------+--------+------------+---------+------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display the content of the DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Establish sensor data payload schema by loading a sample into a data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Data.BAR: string (nullable = true)\n",
      " |-- Data.BAT: string (nullable = true)\n",
      " |-- Data.HDH: string (nullable = true)\n",
      " |-- Data.HDT: string (nullable = true)\n",
      " |-- Data.OPT: string (nullable = true)\n",
      " |-- Data.TMP: string (nullable = true)\n",
      " |-- DeviceID: string (nullable = true)\n",
      " |-- DeviceType: string (nullable = true)\n",
      " |-- Event: string (nullable = true)\n",
      " |-- Time: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(Data.BAR,StringType,true),StructField(Data.BAT,StringType,true),StructField(Data.HDH,StringType,true),StructField(Data.HDT,StringType,true),StructField(Data.OPT,StringType,true),StructField(Data.TMP,StringType,true),StructField(DeviceID,StringType,true),StructField(DeviceType,StringType,true),StructField(Event,StringType,true),StructField(Time,StringType,true)))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "payloadSchema = spark.createDataFrame(json_normalize(json.loads(payloads[0])))\n",
    "payloadSchema.printSchema()\n",
    "payloadSchema.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import relevant data types from pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specify data schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField('Data.BAR', StringType(), True),\n",
    "    StructField('Data.BAT', StringType(), True),\n",
    "    StructField('Data.HDH', StringType(), True),\n",
    "    StructField('Data.HDT', StringType(), True),\n",
    "    StructField('Data.OPT', StringType(), True),\n",
    "    StructField('Data.TMP', StringType(), True),\n",
    "    StructField('DeviceID', StringType(), True),\n",
    "    StructField('DeviceType', StringType(), True),\n",
    "    StructField('Event', StringType(), True),\n",
    "    StructField('Time', TimestampType(), True)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loop through payloads list and add each to an empty Spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time to complete: 0.28s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Define a function to loop through the collection of payloads and add each one to the dataframe\n",
    "def normaliseSparkDataFrame(x):    \n",
    "    #Read first payload element and use to create dataframe\n",
    "    tempdf = spark.createDataFrame(json_normalize(json.loads(payloads[0])))\n",
    "    #Skip the first element and loop through the rest adding each to the dataframe\n",
    "    for i in x [1:]:\n",
    "        row = json_normalize(json.loads(i))\n",
    "        tempdf = tempdf.union(spark.createDataFrame(row))\n",
    "        \n",
    "        #Display individual rows that are added to the dataframe for debugging\n",
    "        #print(row)\n",
    "    \n",
    "    return tempdf\n",
    "\n",
    "#Time execution of the normaliseDataFrame function and assign the result to payloads variable\n",
    "start = time.time()\n",
    "df = normaliseSparkDataFrame(payloads)\n",
    "end = time.time()\n",
    "\n",
    "print(f'\\nTime to complete: {end - start:.2f}s\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+--------+--------+--------+--------+--------+------------+-----+--------------------+\n",
      "|Data.BAR|Data.BAT|Data.HDH|Data.HDT|Data.OPT|Data.TMP|DeviceID|  DeviceType|Event|                Time|\n",
      "+--------+--------+--------+--------+--------+--------+--------+------------+-----+--------------------+\n",
      "| 1016.17|   36.58|   22.65|   36.32|    4.17|  36.187|    8010|Envirosensor|event|2018-06-03 20:40:...|\n",
      "| 1016.14|   35.06|   28.68|   34.76|    0.00|  34.562|    8017|Envirosensor|event|2018-06-01 04:22:...|\n",
      "| 1017.23|   35.49|   26.23|   35.06|    6.17|  34.781|    8008|Envirosensor|event|2018-06-01 13:04:...|\n",
      "| 1013.88|   34.07|   21.58|   34.14|   73.04|  33.843|    8019|Envirosensor|event|2018-05-29 22:48:...|\n",
      "| 1017.59|   35.05|   27.41|   34.77|    6.67|  34.625|    8010|Envirosensor|event|2018-06-01 13:19:...|\n",
      "| 1017.23|   36.19|   18.90|   36.06|    4.89|  35.906|    8004|Envirosensor|event|2018-06-02 23:12:...|\n",
      "| 1018.17|   36.51|   21.66|   36.07|    8.50|  35.812|    8008|Envirosensor|event|2018-06-03 07:33:...|\n",
      "| 1013.19|   34.93|   20.34|   35.01|   74.00|  34.718|    8019|Envirosensor|event|2018-05-30 09:25:...|\n",
      "| 1018.45|   35.87|   22.71|   35.52|  108.72|  35.281|    8005|Envirosensor|event|2018-06-03 00:30:...|\n",
      "| 1018.45|   37.08|   17.05|   36.84|    0.24|  36.500|    8018|Envirosensor|event|2018-06-03 09:34:...|\n",
      "| 1018.69|   36.04|   19.87|   35.83|  111.92|  35.531|    8016|Envirosensor|event|2018-06-03 07:40:...|\n",
      "+--------+--------+--------+--------+--------+--------+--------+------------+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display the content of the DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Resilient Distributed Datasets (RDD's)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load payloads into RDD's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time to complete: 0.01s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Time execution of loading payloads into RDD's\n",
    "start = time.time()\n",
    "sc = spark.sparkContext\n",
    "payloadsRDD = sc.parallelize(payloads)\n",
    "end = time.time()\n",
    "\n",
    "print(f'\\nTime to complete: {end - start:.2f}s\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert RDD to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "processedData = spark.read.json(payloadsRDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+------------+-----+--------------------+\n",
      "|                Data|DeviceID|  DeviceType|Event|                Time|\n",
      "+--------------------+--------+------------+-----+--------------------+\n",
      "|[1016.17, 36.58, ...|    8010|Envirosensor|event|2018-06-03 20:40:...|\n",
      "|[1016.14, 35.06, ...|    8017|Envirosensor|event|2018-06-01 04:22:...|\n",
      "|[1017.23, 35.49, ...|    8008|Envirosensor|event|2018-06-01 13:04:...|\n",
      "|[1013.88, 34.07, ...|    8019|Envirosensor|event|2018-05-29 22:48:...|\n",
      "|[1017.59, 35.05, ...|    8010|Envirosensor|event|2018-06-01 13:19:...|\n",
      "|[1017.23, 36.19, ...|    8004|Envirosensor|event|2018-06-02 23:12:...|\n",
      "|[1018.17, 36.51, ...|    8008|Envirosensor|event|2018-06-03 07:33:...|\n",
      "|[1013.19, 34.93, ...|    8019|Envirosensor|event|2018-05-30 09:25:...|\n",
      "|[1018.45, 35.87, ...|    8005|Envirosensor|event|2018-06-03 00:30:...|\n",
      "|[1018.45, 37.08, ...|    8018|Envirosensor|event|2018-06-03 09:34:...|\n",
      "|[1018.69, 36.04, ...|    8016|Envirosensor|event|2018-06-03 07:40:...|\n",
      "+--------------------+--------+------------+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display the content of the DataFrame\n",
    "processedData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------+--------+------------+-----+--------------------------+\n",
      "|Data                                        |DeviceID|DeviceType  |Event|Time                      |\n",
      "+--------------------------------------------+--------+------------+-----+--------------------------+\n",
      "|[1016.17, 36.58, 22.65, 36.32, 4.17, 36.187]|8010    |Envirosensor|event|2018-06-03 20:40:41.629620|\n",
      "+--------------------------------------------+--------+------------+-----+--------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "processedData.show(1,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------+\n",
      "|Data                                         |\n",
      "+---------------------------------------------+\n",
      "|[1016.17, 36.58, 22.65, 36.32, 4.17, 36.187] |\n",
      "|[1016.14, 35.06, 28.68, 34.76, 0.00, 34.562] |\n",
      "|[1017.23, 35.49, 26.23, 35.06, 6.17, 34.781] |\n",
      "|[1013.88, 34.07, 21.58, 34.14, 73.04, 33.843]|\n",
      "|[1017.59, 35.05, 27.41, 34.77, 6.67, 34.625] |\n",
      "+---------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "processedData.select('Data').show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "\"cannot resolve 'split(`Data`, ',')' due to data type mismatch: argument 1 requires string type, however, '`Data`' is of struct<BAR:string,BAT:string,HDH:string,HDT:string,OPT:string,TMP:string> type.;;\\n'Project [Data#2992, DeviceID#2993, DeviceType#2994, Event#2995, Time#2996, split(Data#2992, ,)[0] AS NAME1#3034]\\n+- LogicalRDD [Data#2992, DeviceID#2993, DeviceType#2994, Event#2995, Time#2996], false\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o1617.withColumn.\n: org.apache.spark.sql.AnalysisException: cannot resolve 'split(`Data`, ',')' due to data type mismatch: argument 1 requires string type, however, '`Data`' is of struct<BAR:string,BAT:string,HDH:string,HDT:string,OPT:string,TMP:string> type.;;\n'Project [Data#2992, DeviceID#2993, DeviceType#2994, Event#2995, Time#2996, split(Data#2992, ,)[0] AS NAME1#3034]\n+- LogicalRDD [Data#2992, DeviceID#2993, DeviceType#2994, Event#2995, Time#2996], false\n\r\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\r\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:115)\r\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:107)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:278)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:278)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:277)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:275)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:275)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:326)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:324)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:275)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:275)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:275)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:326)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:324)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:275)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:104)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$2.apply(QueryPlan.scala:121)\r\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\r\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\r\n\tat scala.collection.immutable.List.foreach(List.scala:392)\r\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\r\n\tat scala.collection.immutable.List.map(List.scala:296)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:121)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:126)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:93)\r\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:107)\r\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:85)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)\r\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:85)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\r\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\r\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\r\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)\r\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:3406)\r\n\tat org.apache.spark.sql.Dataset.select(Dataset.scala:1334)\r\n\tat org.apache.spark.sql.Dataset.withColumns(Dataset.scala:2252)\r\n\tat org.apache.spark.sql.Dataset.withColumn(Dataset.scala:2219)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-61-a208a0831725>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0msplitCol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunctions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocessedData\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Data'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprocessedData\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocessedData\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'NAME1'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msplitCol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetItem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprocessedData\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocessedData\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'NAME1'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msplitCol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetItem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprocessedData\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mwithColumn\u001b[1;34m(self, colName, col)\u001b[0m\n\u001b[0;32m   1987\u001b[0m         \"\"\"\n\u001b[0;32m   1988\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"col should be Column\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1989\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolName\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1990\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1991\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1257\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[0;32m     68\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: \"cannot resolve 'split(`Data`, ',')' due to data type mismatch: argument 1 requires string type, however, '`Data`' is of struct<BAR:string,BAT:string,HDH:string,HDT:string,OPT:string,TMP:string> type.;;\\n'Project [Data#2992, DeviceID#2993, DeviceType#2994, Event#2995, Time#2996, split(Data#2992, ,)[0] AS NAME1#3034]\\n+- LogicalRDD [Data#2992, DeviceID#2993, DeviceType#2994, Event#2995, Time#2996], false\\n\""
     ]
    }
   ],
   "source": [
    "splitCol = functions.split(processedData['Data'],',')\n",
    "processedData = processedData.withColumn('NAME1', splitCol.getItem(0))\n",
    "processedData = processedData.withColumn('NAME1', splitCol.getItem(0))\n",
    "processedData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Data: struct (nullable = true)\n",
      " |    |-- BAR: string (nullable = true)\n",
      " |    |-- BAT: string (nullable = true)\n",
      " |    |-- HDH: string (nullable = true)\n",
      " |    |-- HDT: string (nullable = true)\n",
      " |    |-- OPT: string (nullable = true)\n",
      " |    |-- TMP: string (nullable = true)\n",
      " |-- DeviceID: string (nullable = true)\n",
      " |-- DeviceType: string (nullable = true)\n",
      " |-- Event: string (nullable = true)\n",
      " |-- Time: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(Data,StructType(List(StructField(BAR,StringType,true),StructField(BAT,StringType,true),StructField(HDH,StringType,true),StructField(HDT,StringType,true),StructField(OPT,StringType,true),StructField(TMP,StringType,true))),true),StructField(DeviceID,StringType,true),StructField(DeviceType,StringType,true),StructField(Event,StringType,true),StructField(Time,StringType,true)))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processedData.printSchema()\n",
    "processedData.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
